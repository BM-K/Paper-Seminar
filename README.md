# Paper-Seminar

- ## *Knoweldge Distillation*
  - **Distilling the Knowledge in a Neural Network**
    - https://arxiv.org/pdf/1503.02531.pdf
  - **Improved Knowledge Distillation via Teacher Assistant**
    - https://arxiv.org/pdf/1902.03393.pdf
  - **Knowledge Distillation Meets Self-Supervistion**
    - https://arxiv.org/pdf/2006.07114.pdf
  - **DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter**
    - https://arxiv.org/pdf/1910.01108.pdf
  - **TinyBERT: Distilling BERT for Natural Language Understanding**
    - https://arxiv.org/pdf/1909.10351.pdf
  - **MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers**
    - https://arxiv.org/pdf/2002.10957.pdf
  - **ERNIE-Tiny : A Progressive Distillation Framework for Pretrained Transformer Compression**
    - https://arxiv.org/pdf/2106.02241.pdf

- ## *Sentence Embedding*
  - **SimCSE: Simple Contrastive Learning of Sentence Embeddings**
    - https://arxiv.org/pdf/2104.08821.pdf  
  - **SBERT-WK: A Sentence Embedding Method by Dissecting BERT-based Word Models** 
    - https://arxiv.org/pdf/2002.06652.pdf
  - **Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks**
    - https://arxiv.org/pdf/1908.10084.pdf
  - **Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding**
    - https://www.aclweb.org/anthology/C12-1118.pdf

- ## *Continual Learning*
  - **Overcoming catastrophic forgetting in neural networks**
    - https://arxiv.org/pdf/1612.00796.pdf
  - **Progressive Neural Networks**
    - https://arxiv.org/pdf/1606.04671.pdf
  - **Continual Learning with Deep Generative Replay**
    - https://arxiv.org/pdf/1705.08690.pdf
  - **Continual Learning for Natural Language Generation in Task-oriented Dialog Systems**
    - https://arxiv.org/pdf/2010.00910.pdf

- ## *Video Question Answering*
  - **DramaQA: Character-Centered Video Story Understanding with Hierarchical QA**
    - https://arxiv.org/pdf/2005.03356.pdf
  - **Modality Shifting Attention Network for Multi-modal Video Question Answering**
    - https://arxiv.org/pdf/2007.02036.pdf

- ## *eXpalinable Artificial Intelligence*
  - **Understanding the difficulty of training transformers**
    - https://arxiv.org/pdf/2004.08249.pdf
  - **Do NLP Models Know Numbers? Probing Numeracy in Embeddings**
    - https://arxiv.org/pdf/1909.07940.pdf

- ## *Information Retrieval*
  - **Dense Passage Retrieval for Open-Domain Question Answering**
    - https://arxiv.org/pdf/2004.04906.pdf
  - **ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT**
    - https://arxiv.org/pdf/2004.12832.pdf

- ## *Natural Language Understanding*
  - **ELECTRA pre-training text encoders as discriminators rather than generators**
    - https://arxiv.org/pdf/2003.10555.pdf
  - **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**
    - https://arxiv.org/pdf/1810.04805.pdf
  - **Unifying Vision-and-Language Tasks via Text Generation**
    - https://arxiv.org/pdf/2102.02779.pdf

- ## *Response Generation*
  - **Learning to Copy Coherent Knowledge for Response Generation**
    - https://ojs.aaai.org/index.php/AAAI/article/view/17486
  - **Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks**
    - https://arxiv.org/pdf/2004.01972.pdf
